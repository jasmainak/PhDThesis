\chapter{Conclusion}
\label{chapter:conclusion}

\epigraph{\small\itshape ``The purpose of computation is insight, not  numbers.''}{\small\textit{---Richard Hamming}}

Methods research in neuroimaging is a marriage between computer science and neuroscience. It is a collaboration between two complementary disciplines -- the aim is to bring to the table computation tools which can help scientists make new discoveries. Certain aspects of this interdisciplinary subfield is of course to incrementally develop existing tools: those that can help achieve a better prediction score, or a better localization accuracy in source estimation. However, an orthogonal but equally important aspect of methods research is to develop tools which allow fundamentally new ways to interact with the data. This thesis is an attempt to advance this goal by developing tools for automated analysis in electrophysiology.

It has now become evident to us that in order to achieve the goal of reproducible research, large public datasets are the key and automated methods to analyze them are indispensable. While every neuroscientist's ambition is to generate new insights and push the frontier's of our knowledge of the brain, this is often not possible due to the weak effect sizes which cannot be uncovered in small datasets. When the null hypothesis cannot be rejected, it is a common practice to start fishing for significant results by making multiple comparisons. This has resulted in a body of literature where a large fraction of the results lie on shaky grounds. 

In this thesis, we developed a new specification known as the \ac{BIDS}, which facilitates data sharing between neuroscientists by promoting common standards for storing measurement related metadata. We also provided an overview of the challenges in reproducible data analysis with respect to \ac{MEG}/\ac{EEG} data. As contributors to the MNE software package, we felt particularly well positioned to address  the software related challenges: complex pipelines, software versions, random initialization \emph{etc.,} and standardized recommendations for each stage of these pipelines. We did this by reanalyzing a group study on Faces dataset~\citep{wakeman2015multi}. To ensure reproducible results, the entire analysis was scripted and the plots generated using the \code{sphinx\_gallery}.

In order to even further push the goal of reproducibility via automation: we developed two new methods for analyzing electrophysiological data. The first method, christened \emph{autoreject} aims to streamline the removal of data segments containing artifacts which is a basic preprocessing step in almost every analysis chain. We develop an efficient method which uses a parameter search method known as Bayesian optimization. Our approach was able to facilitate re-analysis of the \ac{HCP} data for benchmarking. Our second method, known as \emph{alphacsc} enables mining neural time series for new oscillatory structures. Not only that, it is a tool to estimate more accurate waveform shapes than what is possible using traditional Fourier analysis. We demonstrated in our work that it was able to discover nested oscillations from the data.

These technologies can still be considered to be in their nascency in many respects. Just as source localization methods in \ac{MEG}/\ac{EEG} have evolved from dipole-based models to distributed methods to more sophisticated models implementing structured sparsity, these new methods are likely to undergo an evolutionary process of incremental improvements. If we consider the example of \ac{CSC}, our model based on alpha-stable distributions extended the computer vision models to be able to handle heavy-tailed distributions that is characteristic in neural data. Obviously, this is not the end of the road. Tuning hyperparameters in \ac{CSC} models is still notoriously difficult, but it is not impossible if there is an supervised task at the end of the pipeline. Multiscale dictionaries might be critical for brain signals considering that the oscillations can have varying support. Even though though it is non-convex, smarter random initialization strategies such as those based on \ac{MCMC} could lead to more accurate estimates~\citep{bachem2016fast}. It will soon be necessary to build streaming \ac{CSC} algorithms based on stochastic approximations to deal with large datasets.

In this thesis, we outlined a strategy for reproducible research in the future: public datasets with large sample sizes and automation. However, the focus of our work was limited to automation on the scale of single subjects. Even though this does enable us to analyse large datasets, it is somewhat limited as any serious estimation based on pooling data across subjects is not tractable. As we enter an era of fast-paced science, such data-driven tools will become indispensable. While a large fraction of methods research, and even the focus of this thesis itself has been on the noise component, this may turn out to be not as important when dealing with larger datasets. The modern approach might in fact prefer large datasets which are not perfectly denoised rather than a smaller perfectly denoised dataset. 

Unlike computer vision or natural language processing, high risk industries such as healthcare require transparent algorithms. It is no longer sufficient to be able to merely achieve higher prediction accuracy. In fact, a large fraction of neuroimaging data, even that which is available publicly, is unlabeled or at best weakly labeled. New generation tools must enable clinicians to rapidly probe the brain to identify signals and structures of interest, quantify uncertainties along with the accuracy scores, perform quality control, and interactively visualize their data.
