\section{Discussion and conclusion}
\label{sec:group_study_discussion}
% Summarize intention and results of the study and goals.
Analyzing M/EEG requires successive operations and transformations on the data. At each analysis stage, the different processing choices can affect the final result in different ways. While this situation encourages tailoring data analysis strategies to the specific demands of the scientific problem, this flexibility comes at a cost and can lead to spurious findings when not handled appropriately~\citep{ioannidis2005,simmons2011false,Carp2012}.
% Indeed, the intrinsic noise of psychological measurements combined with the flexibility of data analysis in neuroimaging and its perennially small sample sizes may suggest a particularly aggravated replication crisis in cognitive neuroscience compared to medical research and psychology~\citep{szucs2017}.
In the absence of fully automated data analysis pipelines that can optimize the choice of processing steps and parameters, it is crucial to develop principled approaches to planning, conducting and evaluating M/EEG data analysis.

The present study makes the effort to elucidate common elements and pitfalls of M/EEG analysis. It presents a fully reproducible group analysis of the publicly available dataset from~\cite{wakeman2015multi}.
All code and results are publicly accessible \url{http://mne-tools.github.io/mne-biomag-group-demo/}. The study provides contextualized in-depth discussion of all major steps of the analysis with regard to alternative options, caveats and quality control measures. As a rare contribution to the M/EEG literature, this study illustrates in comparative figures, the experimental results obtained when changing essential options at different steps in the analysis. In the following, we want to share some insights that we obtained from working together on this study. 

% 1) Lessons learned from this project: collaborative data analysis
\emph{Collaborative data analysis}. In our experience, high-level planning and hands-on data analysis are commonly divided between, e.g., masters or doctoral students, post-docs, and senior researchers. As a consequence, the results are typically appreciated from figures produced without connection to the research code that generated them. In this study, several authors contributed repeatedly to the code, analyses were repeated on different computers, and results were inspected in an ongoing fashion by many authors. This experience has had as consequence that incoherences, model violations, and other quality concerns were perhaps detected more often than usual, which has greatly contributed to the overall quality of the data analysis. While it is perhaps too extreme or onerous to recommend adopting social interaction habits from open source software development---such as peer review, pair or extreme programming---in scientific data analysis, we believe that data analysis should not be done in isolation. In order to enable full-blown collaborative data analysis in research, analysis must be repeatable, hence, scripted, and a minimum of code organization and readability must be enforced. On the other hand, the best coding efforts will have limited impact if there are not multiple authors with fresh and active data analysis habits. We hope that the example stated by this paper, together with the open source tools and the community it is built upon, can stimulate more collaborative approaches in M/EEG research.

% 2) Lessons learned from this project: The costs of reproducibility
\emph{The costs of reproducibility}. It is a commonly neglected reality that reproducibility comes at a price. Making an analysis strictly reproducible not only requires intensified social interactions, hence more time, but also demands more computational resources. It is a combinatorially hard problem if one were to consider all the potential sources of variability. For example, analyses have to be repeated on different computers with different architectures and performance resources. This sometimes reveals differences in results depending on the hardware, operating system, and software packages used. As observed in the past by \cite{glatard-etal:15}, we noticed that some steps in our pipeline such as ICA are more sensitive to these changes, eventually leading to small differences at the end of the pipeline, which is in our case are cluster-level statistics in the source space. Of course, differences due to these changes are harder to control and enforce in the context of today's fast technological progress. Indeed, what we manage to achieve is reproducibility, as opposed to the pure replicability which would be the case if the same results could be achieved even when the computer hardware and software packages were changed.

Also, when code is developed on large desktop computers which are common in many laboratory settings, replication efforts with lower-performance workstations may incur high costs in terms of human processing time. The analysis not only runs slower but may crash, for example due to differences in computer memory resources. We therefore emphasize the responsibility of software developers in providing scalable performance control and the responsibility of hands-on data analysts to design the analysis bearing performance and social constraints in mind. In other words, consider that code needs to run on someone else's computer.


% 3) Lessons learned from this project: when to stop. 
\emph{When to stop?} Obviously, in the light of the current replication crisis, clear rules need to be established on when to stop improving the data analysis~\citep{simmons2011false,szucs2017}. A particular risk is emanating from the possibility of modifying the analysis code to eventually confirm the preferred hypothesis. This would invalidate inference by not acknowledging all the analysis options explored. Apart from commonly recommended preregistration practices and clean hold out data systems, we want to emphasize the importance of quality criteria for developing the analysis. The bulk of M/EEG preprocessing tasks are either implicitly or explicitly model-based, as shown by the rich battery of quality control visualizations presented in this chapter. Such plots allow to assess if M/EEG analysis outputs can be considered good signals. Consequently, analysis should be stopped when no further improvement on quality control metrics is to be expected, within a reasonable time investment. In other words, not research hypotheses (and statistical significance of results) but rather signal quality metrics are the criterion for constructing M/EEG analyses. Ideally, only when quality control is done, should the contrast(s) of interest be investigated.

%XXX OLD TEXT XXX
% XXX I have no idea what is the goal of this section. It seems just a bit redundant. As a reader I feel bored because I cannot smell the direction and I feel like listening to monologue of someone talking. 
% In the preceding sections, we have first detailed the preprocessing steps, including filtering, bad segment detection, artifacts correction, epoching, averaging, source reconstruction, and finally statistics. Visually inspecting individual-channel PSD is easy to do and allows for a preliminary screening of bad channels and potentially strong transient artifacts. As obvious as it may seem, visually inspecting the evoked sensor space data is mandatory and MNE offers convenient routines to visualize topographies and so-called butterfly plots, which overlay evoked potentials for each sensor with lines color-coded by sensor locations. It facilitates easily spotting the remaining eye blink artifacts that affect primarily frontal sensors. Inspecting the butterfly plots for transient artifacts can be useful even for resting state data despite having no evoked response. While MNE offers automated methods to detect bad ICA components, it can sometimes fail and visual inspection of the ICA sources is always a good idea. In terms of source localization, MNE offers the possibility to visualize whitened data to better assess what sources to expect in the brain, and when to look for them. An unexpected whitening result suggests that the candidate noise covariance does not provide a good model of the noise. Possible reasons for this are artifacts in data segments used to estimate the noise covariance, the lack of sufficient data to estimate the covariance, the presence of signal of interest in the data segment, hence, a bad conceptualization of what is noise. This last scenario can occur when working with short inter-stimulus intervals, and its consequence is that whitening tends to reduce or even remove the signal of interest from the post-stimulus data. Like-wise, denoising with SSS and movement-compensation can make it difficult to estimate correctly the true underlying dimensionality of the data, yielding inappropriate spatial degrees of freedom. Most importantly, detecting model violations is the first step to handling them.

% At each stage, the user can choose between alternative strategies: SSS \emph{vs.} tSSS, highpass filtering \emph{vs.} baselining, \emph{etc.}. However, the consequences of these choices are not always clear.

% While it is rarely done in the M/EEG literature, this paper illustrates with comparable figures, the experimental results obtained by changing some key steps in the analysis. Filtering is one important and near-ubiquitous step in analysis pipelines which requires considerable care and attention. For instance, high-pass filtering the data to remove the low frequency and high-variance noise, such as drifts, can affect whether or not we observe significant effects in late periods that tend to contain slowly varying evoked responses. Knowing whether or not to filter and how much is often a difficult decision that is different for each type of study. What our results demonstrated here is that individual subjects' data tend to contain low-frequency responses that eventually return to near-baseline levels by the end of 2900 ms. It is important to note that this upper time limit was set based on the minimum time between stimulus presentations (i.e., the shortest fixation-circle duration and inter-stimulus-interval), so it represents a worst-case lower time limit for activity returning to baseline levels. Taken together, this suggests that high-pass filtering the data is not necessary for these data, and thus is not worth the risk of possibly distorting true sustained responses.

% The analyses conducted here provide some insight into these issues: we tease apart these different dimensions of the data processing pipeline and show their effect on results at the individual subject level and group level. \eric{I don't think there is currently sufficient depth of analysis / options explored to claim this so strongly...}

% Here, we showed how to use MNE to obtain group-level statistics using univariate statistics combined with nonparametric, cluster-based methods. We also explored the use of multivariate decoding methods. By performing state-of-the-art time-decoding at each time point, we were able uncover when the strongest effects occurred, and also report an above-chance decoding accuracy of the contrast much earlier than what the cluster level univariate methods suggest.

% This paper presents a full pipeline from raw data to spatio-temporal analyses of sensor and brain time courses. However, there are a number of other analyses that could have been conducted or which were attempted and did not yield very convincing results. Here we looked primarily at the strongest expected effect, which was the contrast between faces and scrambled faces. Many other contrasts could have been investigated such as unfamiliar \emph{vs.} familiar faces, which was only considered here in the multivariate decoding analysis. Other classic M/EEG analysis not shown here are time-frequency maps of induced power in either sensor or source space. While MNE can be used for such analysis, no clear results were seen in preliminary analyses of sensor space data, which are dominated by the evoked responses. Analyzing power maps to study phenomena such as gamma-band oscillations or high-frequency spiking activity in the visual areas following the face presentations was not attempted, but could be fruitful.

% The size of the dataset analyzed here was between 15 and 20 subjects, which is standard for cognitive neuroscience studies. At this level, visual inspection of the data is time consuming but still possible. It is this visual inspection that led~\cite{wakeman2015multi} to recommend leaving out three subjects out of the analysis. Over the past few years, there has been large community efforts to increase the sample size in brain imaging by pooling more subjects in the studies (see HCP~\citep{van_essen_human_2012,larson2013adding} and Cam-CAN~\citep{taylor2015cambridge}). By pooling more subjects, more subtle effects may be detected. Yet, the analysis time of practitioners cannot grow as the size of the dataset we can now analyze. The consequence of this is that there is certainly a need and a methodological challenge to automate even more steps of the analysis, and to complement this with simple quality control reports. At the same time, we need methods that are robust to the presence of outliers in the cohort of subjects analyzed.

With these broader insights in mind, we will make an attempt to extract from our analysis practical recommendations that should facilitate \emph{future} M/EEG analyses. We encourage the reader not to take the analysis presented here as a direct justifications for parameter choices used in their analyses, but instead learn the principles underlying the choices made in our examples. The general rule is: assess your options and chose the optimal measure at each processing step, then visualize and automate as much as you can.

Practical recommendations:
\begin{enumerate}
\item \textbf{Know your I/O.} Make sure to have a clear idea about the meta-data available
in your recordings and that the software package knows about relevant auxiliary channels, e.g, stim, EOG, ECG.
Use custom MNE functions and other libraries to add quick reading support if I/O for a file-type is 
not readily supported.

\item \textbf{Think noise.} Inspect your raw data and power spectra to see if and how much
denoising is necessary.
When using methods such as SSS, SSP, ICA, or reference-channel correction, be aware of their implications 
for later processing. Remember also to process your empty room data the same way. The 
interpretation of sensor types may change. Denoising may implicitly act as a high-pass filter (cf. tSSS).
High-pass filtering or baselining may not be a good thing, depending on the paradigm. For calibrating your inverse solution, 
think of what is an appropriate noise model, it may be intrinsically linked to your hypothesis. 

\item \textbf{Mind signals of non-interest.} Detect and visualize your physiological artifacts, e.g. ECG, 
EOG, prior to attempting to mitigate them. Choose an option that is precise enough for your data. 
There is no absolute removal, only changes in signal-to-noise ratio. Not explicitly suppressing 
any artifacts may also be a viable option in some situations, whereas a downstream method (e.g., temporal decoding) will not benefit from them. When employing an artifact removal technique,  
visualize how much of your signal of interest is discarded.

\item \textbf{Visually inspect at multiple stages.} Use diagnostic visualizations often to get a sense of signal characteristics, from noise sources, to potential signals of interest. Utilize knowledge of paradigms (e.g., existence of an N100 response) to validate steps. Visual inspection of data quality and SNR is recommended even if the processing is automated. When using the an anatomical pipeline, look at your coregistration and head models to make sure they are satisfactory. Small errors can propagate and induce spurious results. Check for model violations when working with inverse solvers and understand them. Inappropriate noise models will distort your estimated sources in simple or complex ways and may give rise to spurious effects.

\item \textbf{Apply statistics in a planned way.} Averaging data is a type of statistical transformation. Make sure that what you average is actually comparable. To handle the multiple-comparisons problem, different 
options exist. Non-parametric hypothesis-tests with clustering and multivariate decoding are two such options, and they are not mutually exclusive. Keep in mind that \ac{MEG}/\ac{EEG} is primarily about time, not space. A whole-brain approach may or may not be the best thing to pursue in your situation. Anatomical labels may provide an effective way of reducing the statistical search space.

\item \textbf{Be mindful of non-deterministic steps.} To maximize reproducibility, make sure to fix the random initialization of non-deterministic algorithms such as ICA. Not only does it ensure reproducibility, debugging is also easier when the code is deterministic. Prefer automated scripts as opposed to interactive or manual pipelines wherever possible.

\item \textbf{Keep software versions fixed.} In an ideal world, software (and hardware) versions would not matter, as each operation necessary for data analysis should be tested against known results to ensure consistency across platforms and versions. However, this ideal cannot always be met in practice. To limit difficulties, do not change software versions, hardware or operating system versions in the middle of your analysis. Keep in mind that MNE is based on several other pieces of software. Updating them can have an impact on the outcome of MNE routines. Once data analysis is complete, cross-checking on different platforms or with different software versions can be useful for community feedback and identifying fragile or problematic steps.

\end{enumerate}

In order to facilitate the reproduction of all the results presented in this chapter, all the code used to make the figures in this paper, but also much more, is available at \url{http://mne-tools.github.io/mne-biomag-group-demo/}.
