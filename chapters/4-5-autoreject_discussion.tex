\section{Discussion}
\label{sec:discussion}

In this study, we have presented a novel artifact rejection algorithm called \emph{autoreject} and assessed its performance on multiple datasets showing comparisons with other state-of-the-art methods.


We have shown that learning peak-to-peak rejection thresholds subject-wise is justified as the distribution of this statistic indeed varies considerably across subjects. We have shown qualitatively that \emph{autoreject} yielded clean physiological event related field (ERF) and event related potentials (ERP) by correcting or rejecting contaminated data segments. Finally, we have shown quantitavely that \emph{autoreject} yields results closer to the ground truth for more subjects than the algorithms presented in Section~\ref{sec:competing_methods}. We now further discuss the conceptual similarities and differences of our approach to the alternative methods. We also discuss the interaction between \emph{autoreject}
and some other steps in the M/EEG analysis pipelines.

\subsection{Autoreject vs. competing methods}

We believe the key advantage of \emph{autoreject (local)} over the other methods consists in combining data-driven parameter tuning with deterministic and physics-driven data interpolation. This interpolation promotes spatial smoothness of the electric potential on the scalp for EEG, and in the case of MEG, explicitly takes into account the well-understood Maxwell's equations. To recapitulate, the sensor-level thresholds mark outlier segments across trials at the level of individual sensors, following a data augmentation step which exploits the full array of sensors. As trials are seen as independent observations, the thresholds can be therefore learned from the data using cross-validation. The cross-validation is stratified so that each fold contains roughly an equal proportion of the original and augmented trials. At repair time, bad segments are replaced with interpolated data from the good sensors. Of course, this is problematic if the sensor locations are not readily available. Fortunately, it turns out that the sensor positions from standard montages are often good enough for reliable interpolation.

In contrast to \emph{autoreject (local)}, SNS is a purely statistical method that does not take into account the physics of sensor locations for repairing the data. In SNS, the sensors are considered in a leave-one-sensor-out protocol. For each sensor, a ``clean'' subspace is defined from the principal components of the remaining sensors. The data from this sensor is then projected onto the ``clean'' subspace. As we have seen in Section~\ref{sec:results} (Figure~\ref{fig:hcp_scatter}), this does not work satisfactorily, presumably because the SNS method makes strong assumptions regarding the orthogonality of the noise and ``clean'' subspace. The ensuing projection may not improve, and even deteriorate the signal in some cases. The consequence of this is what we observe empirically in Figure~\ref{fig:hcp_scatter}. Applying SNS will also be problematic when multiple sensors are corrupted simultaneously. However, this is less of a problem in the HCP MEG data that we analyzed. 

On the other hand, the FASTER method derives its rejection decisions from multiple signal characteristics. It uses criteria such as between-sensor correlation, variance and power spectrum, by considering their univariate Gaussian statistics with thresholds fixed to a z-score of 3. This default threshold appears to be satisfying as they work on a vast majority of subjects. However, the fact that it does not work as well on certain subjects can limit its adoption for large scale studies. Here, the adaptive nature of threshold detection performed by \emph{autoreject} seems to be a clear advantage.

The RANSAC algorithm also performs adaptive outlier detection, but across sensors rather than trials. While \emph{autoreject (local)} operates on segmented data such as trials time-locked to the stimuli, RANSAC was designed for continuous data without any segmentation. In fact, one could readily obtain bad sensor per trial (as illustrated in Figure~\ref{fig:schematic}) even with RANSAC. However, the authors of the paper did not validate their method on continuous data, and hence, such a modification would require additional work. Although in the case of MEG data, this is not very crucial, this can in fact be critical for EEG data analysis. Remember, that in EEG, one often has to deal with locally bad sensors. And in this context, it is noteworthy that none of the other methods we have discussed so far provides an explicit treatment for single trial analysis in the presence of locally bad sensors. Our comparison to the RANSAC algorithm seems to suggest that the RANSAC algorithm is indeed sensitive to the parameter settings. Even though the default settings appear to work reasonably well for the EEG data (Figure~\ref{fig:dgw_scatter}), they are not so optimal for the HCP MEG data (Figure~\ref{fig:hcp_scatter}).

It is perhaps worth emphasizing that using cross-validation implies that the trials with artifacts are independent. If this assumption is violated and if artifacts are phase-locked between the training and validation sets, \emph{i.e.} occur for all trials at the same time relative to trial onsets, then this can interfere with the estimation procedure in \emph{autoreject}. Another caveat to be noted is that if the data contains more than $\rho^{*}$ (the maximum number of sensors that can be interpolated) bad sensors, and if the trial is not dropped, the data in the remaining bad sensors can still spread to other sensors if one were to use spatial filters such as SSP. Finally, \emph{autoreject} considers only peak-to-peak thresholds for detecting bad sensors. Of course, the user must still mark low-amplitude flat sensors using another threshold; however, a simple threshold would suffice here as such sensors are usually consistently flat.
Regardless of the method that researchers choose to adopt, diagnostic plots and automated reports~\citep{dengemann2015conc} are an essential element to assess and better understand possible failures of automatic procedures. In this regard, transparency of the method in question is important. In the case of our \emph{autoreject (local)} implementation, we offer the possibility for the user to inspect the bad segments marked by the automated algorithm and correct it if necessary. An example of such a plot is shown in Figure~\ref{fig:diagnostic_plot}. Automating the detection of bad sensors and trials has the benefit of avoiding any unintentional biases that might be introduced if the experimenter were to mark the segments manually. In this sense, diagnostic visualization should supplement the analysis by ensuring accountability in the case of unexpected results.

\subsection{Autoreject in the context of ICA, SSP and SSS}

It is now important to place these results in the broader context of electrophysiological data analysis. Regarding the correction of specific artifacts such as \ac{EOG} artifacts, \emph{autoreject (local)} does indeed remove or interpolate some of the trials affected by eye blinks. This is because most eye blinks are not time-locked to the trial onsets and therefore get detected in the cross-validation procedure. However, the weaker eye blinks, particularly those smaller in magnitude than the evoked response, are not always removed. Also, the idea of rejection is to remove extreme values which are supposed to be rare events. This is why our empirical observation suggests that \emph{autoreject (local)} is not enough in the presence of too frequent eye blinks, but also not enough to fully get rid of the smallest EOG artifacts. 

This is where \ac{ICA}~\citep{vigario1997extraction} and Signal Space Projection (SSP)~\citep{uusitalo1997signal} can naturally supplement \emph{autoreject}. These methods are usually employed to extract and subsequently project out signal subspaces governed by physiological artifacts such as muscular, cardiac and ocular artifacts. However the estimation of these subspaces can be easily corrupted by other even more dramatic environmental or device-related artifacts. This is commonly prevented by band-pass filtering the signals and excluding high-amplitude artifacts during the estimation of the subspaces. Both ICA and SSP (particularly if it is estimated from the data rather than an empty room recording) are highly sensitive to observations with high variance. Even though they involve estimating spatial filters that do not incorporate any notion of time, artifacts very localized in time will very likely have a considerable impact on the estimation procedure. This leads us to recommend removing globally bad sensors and employing appropriate rejection thresholds to exclude time segments with strong artifacts. 

The success of applying \emph{autoreject} to any electrophysiological data hinges critically on its ability to isolate artifacts local in time which cannot necessarily be identified by a prototypical spatial signature. However, the spatial interpolation employed by \emph{autoreject} may not be able to repair sensors which are clustered together. In this case, the software package that implements the spatial interpolation should warn the user if the error due to the interpolation is likely to be high. Such a cluster of bad sensors can be expected in the case of physiological artifacts, such as muscular, cardiac or ocular artifacts. To take care of such artifacts with prototypical spatial patterns, ICA is certainly a powerful method, yet manual identification of artifactual components remains today done primarily manually.

If the context of data processing supports estimation of ICA and \ac{SSP} on segmented data, we would recommend to perform it after applying \emph{autoreject}, benefiting from its automated bad sensor and bad trial handling. MEG signals usually contain a strong contribution from environmental electromagnetic fields. Therefore, interference suppression of MEG data is often needed, utilizing hardware and software based approaches (see, \textit{e.g.} \citet{parkkonen:2010} for details). In principle, spatial interpolation of bad sensor signals may not work very well unless the environmental interference has been removed. In the present study, the MNE sample data was recorded in a very well shielded room and did not need separate interference suppression, while the interference in the 4D/BTi data was removed by utilizing the reference channels. Spatial filtering approaches, such as SSP or SSS, may however produce a ``chicken and egg'' dilemma -- whether to apply SSP/SSS or autoreject first - which can be solved using an iterative procedure as suggested by the PREP pipeline~\citep{bigdely2015prep}. That is, first run autoreject only for detection of bad channels but without interpolation. This is followed by an SSS run excluding the bad channels detected by autoreject. Finally, autoreject can be applied on the data free of environmental interference.
%\review{If there are strong environmental artifacts in MEG, they are often dealt with using SSS. In principle, spatial interpolation may not work very well unless the environmental artifacts have been removed. However, we did not encounter this problem when dealing with 4D/BTi data because these artifacts can be easily removed by reference channels which are not corrupted by environmental artifacts. This ``chicken and egg'' dilemma -- whether to apply SSS first or to apply \emph{autoreject} first can be solved using an iterative procedure as suggested by the PREP pipeline~\citep{bigdely2015prep}. That is, first run \emph{autoreject} only for detection but without interpolation. This is followed by an SSS run excluding the bad segments detected by \emph{autoreject}. Finally, \emph{autoreject} can be applied on the data free of environmental artifacts.}

\subsection{Source localization with artifact rejection}

Obviously, artifact-free data benefits almost any analysis that is subsequently performed and the M/EEG inverse problem is no exception. Such benefits not only concern the quality of source estimates but also the choice of source-localization methods, as some of these methods require modification when certain artifact rejection strategies are employed.
As \emph{autoreject} amounts to automating a common, pre-existing and early processing step it does not require changes for source-level analyses. For example, evoked responses obtained using \emph{autoreject (local)} can be readily analyzed with various source localization methods such as beamformer methods~\citep{dalal2008five,gross2001dynamic}, or cortically-constrained Minimum Norm Estimates with $\ell_2$ penalty~\citep{uutela1999visualization}, and noise-normalized schemes, such as dSPM~\citep{dale2000dynamic} and sLORETA~\citep{pascual2002standardized}. 

Certain denoising techniques such as SSP~\citep{uusitalo1997signal} or SSS~\citep{taulu2004suppression} reduce the rank of the data which can be problematic for beamforming techniques~\citep{woolrich2011meg}. This needs special attention, and in some software such as MNE, this is handled using a non-square whitening matrix. However, as \emph{autoreject} does not systematically reduce the rank of the data, it does not even require sophisticated handling of the data rank. At the same time, it works seamlessly with noise-normalization, where the estimation of the between-sensor noise covariance depends on the number of trials. To estimate the noise covariance during baseline periods, one computes the covariance of non-averaged data and then, assuming independence of each trial, the covariance gets divided by the number of trials present in the average~\citep{engemann2015automated_new}. Most existing pipelines scale the covariance by an integer number of trials. In contrast, methods such as robust regression~\citep{diedrichsen2005detecting} that preferentially give less weight to noisy trials, require the noise normalization to be modified. Concretely, one would have to estimate an approximate number of trials or estimate the covariance matrix by restricting the computation to a subset of trials. \emph{Autoreject} does not necessitate any such modifications to the source-localization pipeline, and hence, helps reduce the cognitive load of integration with pre-existing tools.

% Alex : not too found of next paragraph. too short to be clear and raises more questions than needed
% mj : okay, let's remove it for now
% Although we mainly analyzed data from a single modality (EEG or MEG), \emph{autoreject} should be readily applicable to analysis of multimodal data. In that case, the bad trials would be the union of the bad trials for each modality. Moreover, our results are more broadly applicable even though we focused our analysis mostly on automatically setting peak-to-peak thresholds. The cross-validation method could be used to set the threshold for other metrics. However, the user should be wary of the dangers of double dipping~\cite{kriegeskorte2009circular}. Therefore, \emph{autoreject} should be applied on each condition separately and not on the contrast. This is exactly how we analyzed our data in Section~\ref{sec:results} for the EEG faces dataset.

