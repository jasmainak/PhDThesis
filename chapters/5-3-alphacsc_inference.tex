%!TEX root = ../nips_2017.tex



% \paragraph{The M-Step:}
% \label{sec:solve_activation}
\textbf{The M-Step:} 
Given the weights $w_n$ that are estimated during the E-step, the objective of the M-step~\eqref{eq:mstep} is to solve a weighted \ac{CSC} problem, which is much easier when compared to our original problem. 
%
This objective function is not jointly convex in $d$ and $z$, yet it is convex if one fix
either $d$ or $z$.
% Due to non-convexity, we can only guarantee convergence to a local optimum. 
Here, similarly to the vanilla \ac{CSC} approaches~\citep{gips2017discovering,Grosse-etal:2007}, we develop a \emph{block coordinate descent} strategy, where we solve the problem in~\eqref{eq:mstep} for either $d$ or $z$, by keeping respectively $z$ and $d$ fixed.
%
% Since the overall problem is non-convex, one can only aim for obtaining a local optimum. For obtaining a local minimum, several approaches have been proposed \umut{cite}. These methods are based on alternating minimization, where the problem is iteratively solved for either for the atoms while keeping the codes fixed, or vice versa. Both of the two subproblems are convex. \umut{Deficiencies of the algorithms. Algorithms can be made more efficient}
%
% \textbf{Solving for the activations:}
%
We first focus on solving the problem for $z$ while keeping $d$ fixed, given as follows:
\begin{align}
& \min_{z} \sum_{n=1}^{N} \Big( \|\sqrt{w_{n}} \odot (x_{n} - \sum_{k=1}^{K}D^{k} \bar{z}_{n}^{k})\|_{2}^{2} + \lambda \sum_{k}{ \|{z}_{n}^{k} \|_1}\Big) \quad \text{ s.t.  } {z}_n^k \geq 0, \forall n,k\enspace .
\label{eq:problem_definition_z}
\end{align} 
Here, we expressed the convolution of $d^k$ and $z_n^k$ as the inner product of the zero-padded activations $\bar{z}_n^k \triangleq [(z_n^k)^\top, 0 \cdots 0]^\top \in \bbR^{T}_+$, with a Toeplitz matrix $D^k \in \bbR^{T \times T}$, that is constructed from $d^k$.
The matrices $D^k$ are never constructed in practice, and all operations are carried out using convolutions.
This problem can be solved by various constrained optimization algorithms. Here, we choose the quasi-Newton L-BFGS-B algorithm~\cite{byrd1995limited} with a box constraint: $0 \leq z_{n,t}^k \leq \infty$. This approach only requires the simple computation of the gradient of the objective function with respect to $z$ (\textit{cf.} Section~\ref{sec:m-step}). Note that, since trials are mutually independent, we can solve this problem for each $z_n$ in parallel. 

% \subsection{Solving for atoms}
We then solve the problem for the atoms $d$ while keeping $z$ fixed. This optimization problem turns out to be a constrained weighted least-squares problem. In the non-weighted case, this problem can be solved either in the time domain or in the Fourier domain~\citep{Grosse-etal:2007,heide2015fast,wohlberg2016efficient}. The Fourier transform simplifies the convolutions that appear in least-squares problem, but it also induces several difficulties, such as that the atom $d_k$ have to be in a finite support $L$, an important issue ignored in the seminal work of~\cite{Grosse-etal:2007} and addressed with an \ac{ADMM} solver  in \cite{heide2015fast,wohlberg2016efficient}.
In the weighted case, it is not clear how to solve this problem in the Fourier domain. We thus perform all the computations in the time domain.


%The efficiency of the M-step is crucial in our applications, since it will be run several times within a single run of the overall algorithm. Since we will be considering relatively long observations and rather short atoms in our applications, the approaches that operate in the Fourier domain turns out to be inefficient due to the additional constraints that appear only in the frequency domain. Therefore, we perform all the computations in the time domain in order to improve computational efficiency.



% The performances of our strategy will be demonstrated in benchmarks in the next section. for estimating finite impulse response filters  \in \bbR^{T-L+1}
Following the traditional filter identification approach~\citep{moulines1995subspace}, we need to embed the one-dimensional signals $z_n^k$ into a matrix of delayed signals $Z_n^{k} \in \bbR^{T \times L}$, where $ (Z_n^{k})_{i,j} = z_{n,i + j - L + 1}^k$ if $ L - 1 \le i+j < T$ and $0$ elsewhere. Equation~\eqref{eq:problem_definition} then becomes:
%
\begin{align}
& \min_{d} \sum_{n=1}^{N} \|\sqrt{w_n} \odot (x_{n} - \sum_{k=1}^{K}Z_{n}^{k}d^{k})\|_{2}^{2}, \quad \text{  s.t.  } \|d^k\|_2^2 \leq 1 \enspace.
\label{eq:problem_definition_d}
\end{align}
%
Due to the constraint, we must resort to an iterative approach. The options are to use (accelerated) projected gradient methods such as FISTA~\citep{beck2009fast} applied to~\eqref{eq:problem_definition_d}, or to solve a dual problem as done in~\cite{Grosse-etal:2007}. The dual is also a smooth constraint problem yet with a simpler positivity box constraint (\textit{cf.} Section~\ref{sec:m-step}). The dual can therefore be optimized with L-BFGS-B. Using such a quasi-Newton solver turned out to be more efficient than any accelerated first order method in either the primal or the dual (\textit{cf.} benchmarks in Section~\ref{sec:m-step-performance}).
% As in existing literature\cite{}, we first attempted to solve this problem by transforming Equation~\ref{eq:problem_definition} into the frequency domain using a Fourier transform so as to change the convolution into an elementwise multiplication. In fact, this can greatly speed up the computation, but for one crucial fact: truncating the higher order coefficients in the inverse Fourier transform of the estimated atom can make the objective function non-monotonic in the presence of noise. It turns out that the correct approach involves a constraint on the support of the atoms which does not offer similar computational benefits (as will be seen in our Results section).
%




Our entire \ac{EM} approach can be summarized in the Algorithm~\ref{alg:alpha_csc}.
Note that during the alternating minimization, thanks to convexity we can warm start the $d$ update and the $z$ update using the solution from the previous update. This significantly speeds up the convergence of the L-BFGS-B algorithm, particularly in the later iterations of the overall algorithm. We will not describe the E step and the M steps in more detail.

% \mjtodo{We don't talk about warm starting the iterations of EM anywhere. Maybe not very important ...}


\subsection{Details of the E-Step}
\label{sec:e-step}
%\umut{I will put more derivations here.}

Computing the weights that are required in the M-step requires us to compute the expectation of $\frac1{\phi_{n,t}}$ under the posterior distribution $p(\phi_{n,t}|x,d,z)$, which is not analytically available. 

Monte Carlo methods are numerical techniques that can be used to approximately compute the expectations of the form:
\begin{align}
\mathds{E}[f(\phi_{n,t})] = \int f(\phi_{n,t}) \pi(\phi_{n,t}) d\phi_{n,t} \approx \frac1{J} \sum_{j=1}^J f(\phi_{n,t}^{(j)}) \label{eqn:mc}
\end{align}
where $\phi_{n,t}^{(j)}$ are some samples drawn from $\pi(\phi_{n,t}) \triangleq p(\phi_{n,t}|x,d,z)$ and $f(\phi) = 1/\phi$ in our case. However, in our case, sampling directly from $\pi(\phi_{n,t})$ is also unfortunately intractable.

%  through a transition kernel;  $ \mathbf{\Theta}^{(i+1)} \sim {\cal T}(\mathbf{\Theta}|\mathbf{\Theta}^{(i)})$

\ac{MCMC} methods generate samples from the target distribution $\pi(\phi_{n,t})$ by forming a Markov chain, whose stationary distribution is $\pi(\phi_{n,t})$, 
%
so that $\pi(\phi_{n,t}) = \int {\cal T}(\phi_{n,t}|\phi_{n,t}') p(\phi_{n,t}') d\phi_{n,t}'$, where ${\cal T}$ denotes the transition kernel of the Markov chain. 

In this study, we develop a \ac{MH} algorithm, that implicitly forms a transition kernel. 
%
The MH algorithm generates samples from a target distribution $\pi(\phi_{n,t})$ in two steps. First, it generates a random sample $\phi_{n,t}'$ from a \emph{proposal} distribution $\phi_{n,t}' \sim q(\phi_{n,t}'|\phi_{n,t}^{(j)})$, then computes an acceptance probability $\text{acc}(\phi_{n,t}^{(j)} \rightarrow \phi_{n,t}')$ and draws a uniform random number $u \sim {\cal U}([0, 1])$. If $u < \text{acc}(\phi_{n,t}^{(j)} \rightarrow \phi_{n,t}')$, it accepts the sample and sets $\phi_{n,t}^{(j+1)} = \phi_{n,t}'$; otherwise it rejects the sample and sets $\phi_{n,t}^{(j+1)} = \phi_{n,t}^{(j)}$. The acceptance probability is given as follows
\begin{align}
\text{acc}(\phi_{n,t} \rightarrow \phi_{n,t}') 
&= \min \Bigr\{1, \frac{q(\phi_{n,t}|\phi_{n,t}') \pi(\phi_{n,t}')}{q(\phi_{n,t}'|\phi_{n,t}) \pi(\phi_{n,t})}\Bigr\} \nonumber \\
&= \min \Bigr\{1, \frac{q(\phi_{n,t}|\phi_{n,t}') p(x_{n,t}|\phi_{n,t}',d,z) p(\phi_{n,t}') }{q(\phi_{n,t}'|\phi_{n,t}) p(x_{n,t}|\phi_{n,t},d,z) p(\phi_{n,t}) }\Bigr\}
\end{align}
where the last equality is obtained by applying the Bayes rule on $\pi$. 

The acceptance probability requires the prior distribution of $\phi$ to be evaluated. Unfortunately, this is intractable in our case since this prior distribution is chosen to be a positive $\alpha$-stable distribution whose PDF does not have an analytical form. As a remedy, we choose the prior distribution of $\phi_{n,t}$ as the proposal distribution, such $q(\phi_{n,t}|\phi_{n,t}') = p(\phi_{n,t})$. This enables us to simplify the acceptance probability. Accordingly, for each $\phi_{n,t}$, we have the following acceptance probability:
\begin{align}
  \text{acc}(\phi_{n,t}^{(i,j)} \rightarrow \phi_{n,t}' ) \triangleq \min  \Bigl\{1, \exp(\log \phi_{n,t}^{(i,j)} - \log \phi_{n,t}')/2 + (x_{n,t} - \hat{x}^{(i)}_{n,t})^2 (1/{\phi_{n,t}^{(i,j)}} - 1/{\phi_{n,t}'}) \Bigr\}.
\end{align}
Thanks to the simplification, this probability is tractable and can be easily computed. 

\subsection{Details of the M-Step}
\label{sec:m-step}
\paragraph{Solving for the activations: }
In the M-step, we optimize~\eqref{eq:problem_definition_z} to find the activations $z_n^{(i)}$ of each trial $n$ independently. To keep the notation simple, we will drop the index for the iteration number $i$ of the EM algorithm.

First, this equation can be rewritten by concatenating the Toeplitz matrices for the $K$ atoms into a big matrix $D = [D^1, D^2, ..., D^K] \in \bbR^{T \times KT}$ and the activations for different atoms into a single vector $\bar{z}_n = [(\bar{z}_n^1)^\top, (\bar{z}_n^2)^\top, ..., (\bar{z}_n^K)^\top]^\top \in \bbR^{KT}_+$ where $(\cdot)^\top$ denotes the transposition operation. Recall that $\bar{z}_n^k$ is a zero-padded version of $z_n^k$. This leads to a simpler formulation and the objective function $\mathcal{L}(d, z)$:
\begin{equation}
\mathcal{L}(d, z) = \sum_{n=1}^{N} \frac{1}{2}\|\sqrt{w_{n}} \odot (x_{n} - D \bar{z}_{n})\|_{2}^{2} + \lambda \mathbbm{1}^\top \bar{z}_{n} \enspace ,
\label{eq:problem_definition_d_simple}
\end{equation}
where $\mathbbm{1} \in \bbR^{KT}$ is a vector of ones.

The derivative w.r.t. $z_n$ now reads:
\begin{equation}
\frac{\partial \mathcal{L}(d, z)}{\partial \bar{z}_n}
= D^\top(w_n \odot (x_n - D\bar{z}_n)) + \lambda \mathbbm{1}^\top \enspace .
\end{equation}
In practice, this big matrix $D$ is never assembled and all operations are carried out using convolutions. Note also that we do not update the zeros from the padding in $\bar{z}_n^k$. Now that we have the gradient, the activations can be estimated using a efficient quasi-Newton solver such as L-BFGS-B, taking into account the box posititivy constraint $0 \leq z_n \leq \infty$.
%In the case of PG algorithm, one has a modified soft thresholding operator given by a rectified linear function $\mathcal{S}_{\lambda t}=\mathrm{max}(0, z_i - \lambda t)$.
%The step size for ISTA and FISTA can be chosen to be the inverse of the Lipschitz constant which is equal to the largest eigenvalue of $D^{\top}(\tau_{i} \odot D)$. In practice, this is estimated using the power iteration method \mainak{[ref]} with warm restarts.

For each trial, one iteration costs $\mathcal{O}(LKT)$.

\paragraph{Solving for the atoms: }

In the M-step, we optimize \eqref{eq:problem_definition_d} to find the atoms $d^k$.
As when solving for the activations $z_n$, we can remove the summation over the atoms by concatenating the delayed matrices into $Z_{n}=[Z_{n}^1, Z_{n}^2, \dots, Z_{n}^K] \in \bbR^{T \times KL}$ and $d=[(d^1)^\top, (d^2)^\top, ..., (d^K)^\top]^\top \in \bbR^{KL}$. This leads to the simpler formulation:
%
\begin{align}
& \min_{d} \sum_{n=1}^{N} \frac{1}{2}\|\sqrt{w_n} \odot (x_{n} - Z_{n}d)\|_{2}^{2}, \quad \text{  s.t.  } \|d^k\|_2^2 \leq 1 \enspace.
\label{eq:subproblem_d}
\end{align}
%
%If not for the constraint, this is a classic least square problem which has a known closed-form solution:
%\begin{equation}
%\hat{d} = \Big(\sum_{i=1}^N Z_i^{\top}(\tau_{i} \odot Z_i) \Big)^{-1}\sum_{i=1}^{N} (\tau_{i} \odot Z_i)^{\top}x_i
%\end{equation}
%\ag{say somewhere that FFT based cross-correlation is not possible anymore with EM sample weights}
%\ag{we need to keep the maths for what we are doing and put all the rest in appendix. No need to remind text book notions of optim in main text.}
%
 The Lagrangian of this problem is given by:
%
\begin{equation}
g(d, \beta) = \sum_{n=1}^{N} \frac{1}{2}\|\sqrt{w_n} \odot (x_{n} - \sum_{k=1}^{K} Z_{n}^{k}d^{k}) \|_{2}^{2} + \sum_k \beta^k (\|d^k\|_2^{2} - 1) \quad \text{s.t. } \beta^k \geq 0 \enspace,
\end{equation}
%
where $\beta = (\beta^1, \beta^2, ..., \beta^K)$ are the dual variables. Therefore, the dual problem is:
%
\begin{align}
\min_{d}{g(d, \beta)} = g(d^{*}, \beta)
\end{align}
%
where $d^*$, the primal optimal, is given by:
%
\begin{equation}
d^{*} = (\sum_{n=1}^N Z_n^{\top}(w_{n} \odot Z_n) + \bar{\beta} )^{-1}\sum_{n=1}^{N}(w_{n} \odot Z_n)^{\top}x_n
\label{eq:dual_optimal}
\end{equation}
%
with $\bar{\beta} = \mathrm{diag}([\mathbbm{1}\beta^1, \mathbbm{1}\beta^2, ..., \mathbbm{1}\beta^K]) \in \bbR^{KL}$ with $\mathbbm{1} \in \bbR^{L}$. The gradient for the dual variable $\beta^k$ is given by:
%
\begin{equation}
\frac{\partial g(d^{*}, \beta)}{\partial \beta^k}  = \|{d^{*}}^k\|_2^2 - 1,
\end{equation}
%
with ${d^{*}}^k$ computed from~\eqref{eq:dual_optimal}.
%Of course, we do not know the primal optimal $\hat{d}^k$ offhand, as it is what we want to estimate.
We can solve this iteratively using again L-BFGS-B taking into account the positivity constraint $\beta^k \geq 0$ for all $k$.
% It amounts to computing the primal update at each step, then the dual gradients according to the updated primal, then updating the dual using the gradient and continuing this way until convergence. \mainak{Is there a name for this class of algorithms, so the reader can easily relate?}
What we have described so far solves for all the atoms simultaneously. However, it is also possible to estimate the atoms sequentially one at a time using a block coordinate descent (BCD) approach, as in the work of \citep{mairal2010online}. In each iteration of the BCD algorithm, a residual $r_n^k$ is computed as given by:
%
\begin{equation}
r_n^k = x_n - \sum_{k'\neq k} Z^{k'}_{n}d^{k'}
\end{equation}
%
and correspondingly subproblem \ref{eq:subproblem_d} becomes:
%
\begin{align}
& \min_{d^k} \sum_{n=1}^{N} \frac{1}{2}\|\sqrt{w_n} \odot (r^k_{n} - Z^k_{n}d^k)\|_{2}^{2}, \quad \text{  s.t.  } \|d^k\|_2^2 \leq 1, \enspace.
\label{eq:subproblem_d_block}
\end{align}
%
%Solving for atoms can be done all atoms simultaneously or using a block coordinate descent approach by looping sequentially over atoms.
which is solved in the same way as subproblem~\ref{eq:subproblem_d}. Now, in the simultaneous case, we construct one linear problem in $\mathcal{O}(L^2K^2TN)$ and one iteration costs $\mathcal{O}(L^3K^3)$. However, in the BCD strategy, we construct $K$ linear problems in $\mathcal{O}(L^2TN)$ and one iteration costs only $\mathcal{O}(L^3)$.
Interestingly, when the weights $w_n$ are all identical, we can 
use the fact that for one atom $k$, the matrix $\sum_{i=1}^{N}(Z_i^k)^T Z_i^k$ is Toeplitz. In this case, we can construct $K$ linear problems in only $\mathcal{O}(LTN)$ and one iteration costs only $\mathcal{O}(L^2)$.

For the benefit of the reader, we summarize the complexity of the M-step in Table~\ref{table:complexity_m}. We note $p$ and $q$ the number of iterations in the L-BFGS-B methods for the activations update and atoms update.


% \subsection{Computational tricks and complexity analysis}
\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Method & Complexity \\
\hline
Solving activations $z$ & $p\min(L, \log(T))KTN$ \\
%\hline
%z update LBFGS-B & $\min(pLKTN, p\log(T)KTN)$ \\
%\hline
%d update primal & $L^2K^2TN + qL^2K^2$ \\
%\hline
Solving atoms $d$ & $L^2K^2TN + qL^3K^3$ \\
%\hline
%d update block primal & $LKTN + qL^2K$ \\
%\hline
Solving atoms $d$ (sequential) & $LKTN + qL^2K$ \\
\hline
\end{tabular}
\vspace{5pt}
\caption[Complexity analysis of the M-step.]{Complexity analysis of the M-step, where $p$ and $q$ are the number of iterations in the L-BFGS-B solvers for the activations and atoms updates.}
\label{table:complexity_m}
\end{center}
\end{table}





