\chapter*{Résumé}

Les expériences d'électrophysiologie ont longtemps reposé sur de petites cohortes de sujets pour découvrir des effets statistiquement significatifs d'intérêt. Toutefois, la faible taille de l'échantillon se traduit par une faible puissance, ce qui entraîne un taux élevé de fausses découvertes et, par conséquent, un faible taux de reproductibilité. Pour résoudre ce problème, il faut résoudre deux problèmes connexes: premièrement, comment faciliter le partage et la réutilisation des données pour créer de grands ensembles de données; et deuxièmement, une fois que de grands ensembles de données sont disponibles, quels outils pouvons-nous construire pour les analyser?

Dans la première partie de la thèse, nous introduisons une nouvelle norme de données pour le partage des données connue sous le nom de Brain Imaging Data Structure (BIDS), et son extension MEG-BIDS. Ensuite, nous présentons au lecteur un pipeline électrophysiologique typique analysé avec le progiciel MNE. Cela les orientera vers le processus d'analyse et les défis qui sont souvent rencontrés lors de la construction de pipelines reproductibles. Nous tenons compte des différents choix que les utilisateurs doivent faire à chaque étape du pipeline et nous formulons des recommandations normalisées.

Ensuite, nous concentrons notre attention sur les outils permettant d'automatiser l'analyse de grands ensembles de données tels que ceux offerts par le projet Connectome humain (HCP). Nous proposons un outil automatisé pour supprimer les segments de données corrompus par des artefacts. Nous développons un algorithme de détection d'anomalies basé sur le réglage des seuils de rejet avec recherche de paramètres par optimisation bayésienne. Plus important encore, nous utilisons les données HCP, qui sont annotées manuellement, pour comparer notre algorithme aux méthodes de pointe existantes. À notre connaissance, il s'agit du premier cas de réanalyse de l'ensemble de données à l'aide d'une pile d'outils indépendants utilisés par le consortium HCP.

Enfin, nous utilisons le codage convolutionnel à faible densité pour découvrir les structures des séries chronologiques neuronales. La méthode que nous proposons s'inspire d'algorithmes similaires en vision par ordinateur, où le but est d'apprendre les coefficients d'un dictionnaire d'atomes (traditionnellement sinusoïdaux ou ondelettes) mais aussi les atomes eux-mêmes. Nous reformulons l'approche existante comme une inférence MAP pour être en mesure de faire face aux artefacts d'amplitude élevée et aux distributions de bruits lourds qui sont si courantes dans les séries chronologiques neuronales. 

Ensemble, cette thèse représente une tentative de passer de méthodes d'analyse lentes et manuelles à des méthodes d'analyse automatisées et reproductibles.
