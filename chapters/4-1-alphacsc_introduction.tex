\chapter{Temporal representation learning}
\label{chapter:alphacsc}

\epigraph{\small\itshape \hspace{100pt} ``Sparse is better than dense.''}{\small\textit{---The Zen of Python}}

\begin{figure}[ht!]
\centering
\begingroup
\etocstandardlines
%\renewcommand{\etocbkgcolorcmd}{\color{lightgray}}
\renewcommand{\etocbelowtocskip}{0pt\relax}
\fboxsep1ex
\etocframedstyle [1]{\fbox{\makebox[.4\linewidth]{\etocfontminusone
Contents}}}
\localtableofcontents
\endgroup
\end{figure}

% \etocruledstyle[1]{Contents} \localtableofcontents
\clearpage
\section{Background}

Representations are the building blocks of signal processing. It is quite easy to convince ourselves of this fact, if we simply consider the example of low-pass filtering. When we low-pass filter the data, we are in effect, decomposing it into a sum of sinusoids of varying frequency, and then suppressing the high-frequency components. If on the other hand, we are interested in a time-frequency analysis, the choice of representation would be Morlet wavelets rather than sinusoids.

Traditionally, the choice of representation has been mainly driven by analytical concern and ease of mathematical manipulation. However, the recent surge of deep learning has ignited an interest in data-driven representations. It is because good representations or features that compactly capture the properties of the data, are essential for efficient and accurate learning systems. In computer vision, for instance, handcrafted features such as SIFT~\citep{lowe1999object} and GIST descriptors~\citep{oliva2001modeling}, Deformable Parts Model (DPM)~\citep{felzenszwalb2010object}, Histogram of Oriented Gradient (HOG)~\citep{dalal2005histograms} \emph{etc.} had been the norm, before it was realized that unsupervised learning and autoencoders performed much better.

Today, unsupervised learning is used as a first step for a supervised learning task in computer vision. Representation learning, by itself, is perhaps not as interesting, except for diagnostic purposes~\citep{zeiler2014visualizing}. Despite this, there has always an interest in understanding representations in the human brain (visual system particularly), as it was thought that this would help us build better learning systems. The earliest pioneer in this area of research is Bruno Olshausen, whose work on dictionary learning~\citep{olshausen1996emergence} demonstrated that Gabor patches are indeed fundamental to natural images, similar to the ones that Hubel and Wiesel~\citep{hubel1962receptive, marcelja1980mathematical} found in the cat visual cortex, and what is used in GIST features. 

Barring this finding, the representation itself is usually not as meaningful as the prediction score or reconstruction loss. However, we realized that, in the case of neural signals, this is not the case and the fidelity of the representation is in itself interesting as the shape is crucial in many clinical applications~\citep{cole2017brain}. Before approaching this problem, we will give a brief overview of dictionary learning, its evolution into shift-invariant sparse coding, and parallel attempts in the neuroscience community.

\subsection{Dictionary learning}

A dictionary is a set of atoms (also known as filters) which can be combined with certain coefficients to approximate the data. As such, there is no requirement of orthogonality on the atoms. To learn a representation of the data boils down to estimating the coefficients used in the approximation, which are often assumed to be sparse. Sparsity can be promoted by using an $\ell_0$ or $\ell_1$ penalty, which helps maintain a compact representation. In the traditional approach, it is quite common to start off with an \emph{overcomplete} dictionary, \emph{i.e.,} to have more atoms than would be needed, and only estimate the coefficients. This can be done, for example, using matching pursuit~\citep{mallat1993matching}, which is a greedy algorithm for sparse approximations.

It is easy to see that this approach is memory intensive due to the overcompleteness of the dictionary, but also this approach requires making assumptions about the shape of the atoms in the dictionary. In the present day, it is more common to learn the dictionary and the atoms itself, and so this is known as \emph{dictionary learning}. Dictionary learning can be thought of as a data decomposition method like matrix factorization [ref], but with a sparse regularization. Typically, the convex relaxation of the $\ell_0$ penalty, the $\ell_1$ penalty is used in these situations. Here, the atoms form the columns of the dictionary matrix. The problem is biconvex after the relaxation, and can therefore be solved by alternate minimization. Dictionary learning is now being used for denoising [ref], inpainting~\citep{mairal2009online}, and classification~\citep{mairal2009supervised}. 

\subsection{Coding for shift invariance}
In traditional dictionary learning, the signal or the image is divided into patches and these patches are used as samples for the learning. The main disadvantage of this method is that it results in redundant atoms that are shifted versions of each other. This is the reason a shift-invariant version of dictionary learning is needed. One of the earliest paper in this regard can be credited to \cite{lewicki1999coding}. In their work, the dictionary was fixed and the shift-invariance was encoded using convolutions. For the estimation, an overcomplete dictionary in the shape of a Toeplitz matrix is constructed by shifting the atoms across time. More recent work by \cite{grosse2012shift} made use of the Fourier transform to solve the problem in Frequency domain and compute an inverse transform of the learned dictionary in the end. Another approach that has proved to be quite efficient is the so-called predictive sparse coding which uses neural networks~\citep{kavukcuoglu2010learning}. It is hard to summarize all the work in this area, but it suffices to say that shift-invariant dictionary learning has been gaining popularity in audio signals, images, music, and neural data (See Section~\ref{sec:alphacsc_intro} for a more comprehensive list of references).

Even so, most work so far in this space has proposed new methods, and not much thought has been given to the evaluation of these algorithms -- both computationally as well as qualitatively. For example, it is not even clear if using the Fourier transform has any computational advantage over using a simple Toeplitz matrix. Evaluating the computational complexity of these models is not so straightforward for a number of reasons. First, the problem is non-convex, and as a result, the convergence depends on the initialization. Second, the overall speed depends on the convergence rate, but also on the complexity of each iteration. Finally, the optimization procedure is nested. At each step of the alternate minimization, the solver for the activations or the atoms are solved only for a few iterations rather than up to machine precision. It is because of this that in our work~\citep{jas2017learning}, we focus more on the experimental evaluation of speed, rather than a theoretical comparison of complexity. Even our qualitative analysis goes beyond the narrative of verifying the existence of known waveforms to uncovering more complex structures in the data. 

\subsection{The neuroscientific viewpoint}

A parallel development in the field of neuroimaging has been the rise in interest for learning prototypical shapes which are shift invariant~\citep{jost2006motif, barthelemy2013multivariate, brockmeier2016learning, hitziger2017adaptive}. It is motivated by the fact that existing approximations using the Fourier basis often distorts the signal. There is, for example, a debate regarding the type of filters that should be used (See Section~\ref{sec:group_study_temporal_filtering} and \cite{widmann2015digital,parks1987digital,ifeachor2002digital, gotz-etal:15}). However, there has been so far very little cross-pollination of ideas between the sparse coding and neuroimaging communities. Our work is an attempt to bridge this gap. We propose a model which builds upon existing shift-invariant sparse coding models to be able to handle heavy-tailed noise. It assumes positivity of the coefficients to account for the fact that an atom does not change polarity over time. The upcoming sections describe how this could be achieved.

\subsection{Solvers}

%!TEX root = ../nips_2017.tex

\noindent\fcolorbox{white}{lightgray}{%
\begin{minipage}{\dimexpr\textwidth-2\fboxrule-2\fboxsep\relax}
\begin{itemize}[align=left, leftmargin=10pt, labelwidth=5pt, labelindent=10pt, itemsep=5pt, topsep=5pt]
  \item[] Section~\ref{sec:alphacsc_intro} to Section~\ref{sec:alphacsc_conclusion} was published in:
  \item \bibentry{jas2017learning}
\end{itemize}
\end{minipage}}%

\clearpage

\section{Introduction}
\label{sec:alphacsc_intro}
Neural time series data, either non-invasive such as \ac{EEG} 
% \utodo{we shouldn't use abbreviations if they won't be used later in the text.}
or invasive such as \ac{ECoG} and \acp{LFP}, are fundamental to modern experimental neuroscience. Such recordings contain a wide variety of `prototypical signals' that range from beta rhythms (12--30 Hz) in motor imagery tasks and alpha oscillations (8--12 Hz) involved in attention mechanisms, to spindles in sleep studies, 
% \utodo{Does everybody know what this is?}
and the classical P300 event related potential, a biomarker for surprise. 
%
These prototypical waveforms are considered critical in clinical and cognitive research~\citep{cole2017brain}, thereby motivating the development of computational tools for learning such signals from data.
%\mjtodo{Stylistic remark: This is the same as first sentence in abstract.}


Despite the underlying complexity in the morphology of neural signals, the majority of the computational tools in the community are based on representing the signals with rather simple, predefined bases, such as the Fourier or wavelet bases~\citep{cohen2014analyzing}.
While such bases lead to computationally efficient algorithms, they often fall short at capturing the precise morphology of signal waveforms, as demonstrated by a number of recent studies~\citep{jones2016brain,mazaheri2008asymmetric}. An example of such a failure is the disambiguation of the alpha rhythm from the mu rhythm~\citep{hari2017meg}, both of which have a component around $10$\,Hz but with different morphologies that cannot be captured by Fourier- or wavelet-based representations.

% with logarithmic complexities --> logarithmic in what?


 % can serve as biomarkers for pathologies or modified by certain cognitive tasks~\cite{cole2017brain,Cole4830}. 

% For example, filtering a signal in a relatively narrow band transforms it to an almost perfect sinusoid. This issue is reflected in many recent studies~\cite{tanner2015inappropriate,rousselet2012does,acunzo2012systematic} which have debated the pros and cons of filtering in analysis pipelines because it can ``distort'' waveforms. When looking at bursts of oscillations, typically using Morlet wavelets, the asymmetric shape of an alpha wave~\cite{mazaheri2008asymmetric} or the ``sawtooth'' shape of a beta oscillations~\cite{cole2017brain} are very often invisible. Disambiguating the alpha rhythm from the mu rhythm~\cite{hari2017meg}, both of which have a component around 10 Hz, while having different morphologies is also not doable when looking at amplitude of Fourier-based representations.

%~\cite{durka2005multichannel,jost2006motif,sieluzyckiamultivariate,benar2009consensus,barthelemy2013multivariate}
% These observations is what has spurred the recent 

Recently, there have been several attempts for extracting more realistic and precise morphologies directly from unfiltered electrophysiology signals, via dictionary learning approaches \citep{jost2006motif,brockmeier2016learning,hitziger2017adaptive,gips2017discovering}.
%
These methods all aim to extract certain \emph{shift-invariant} prototypical waveforms (called `atoms' in this context) to better capture the temporal structure of the signals.
%
As opposed to using generic bases that have predefined shapes, such as the Fourier or the wavelet bases, these atoms provide a more meaningful representation of the data and are not restricted to narrow frequency bands.
% would be able to capture the temporal structure of the signals via the shift-invariance, 
% The first attempts \cite{durka2005multichannel,sieluzyckiamultivariate,benar2009consensus, brockmeier2016learning} used matching pursuit to select the best atoms from an overcomplete dictionary of atoms.

% In this line of research, \citet{jost2006motif} proposed the MoTIF algorithm, which uses an iterative strategy based on generalized eigenvalue decompositions, where the atoms are assumed to be orthogonal to each other. Even though this algorithm has been shown to be successful at estimating the alpha waves from EEG, it cannot be applied to signals where the atoms can be correlated, a case which often occurs in practical applications. 
% %
% More recently, the Sliding Window Matching algorithms~\cite{gips2017discovering} were proposed for learning the time-varying atoms from electrophysiological recordings. \umut{Even though these algorithms have ..... advantages,} 
% \utodo{Alex, I let you fix this.}
% % \mainak{It uses correlations to estimate recurring patterns, but it cannot handle atoms that have varying amplitudes or are overlapping.} 
% they cannot handle atoms that can be simultaneously activated or atoms with varying amplitudes. On the other hand, this approach is based on an heuristics-based inference technique which does not explicitly solve a mathematical objective\mjtodo{We should be careful with this statement. SWM does have a mathematical objective function that they minimize.} and therefore cannot be easily extended or improved due to this heuristic nature. 

In this line of research, \citet{jost2006motif} proposed the MoTIF algorithm, which uses an iterative strategy based on generalized eigenvalue decompositions, where the atoms are assumed to be orthogonal to each other and learnt one by one in a greedy way.
%
More recently, the `sliding window matching' (SWM) algorithm \citep{gips2017discovering} was proposed for learning time-varying atoms by using a correlation-based approach that aims to identify the recurring patterns. Even though some success has been reported with these algorithms,
they have several limitations: SWM uses a slow stochastic search inspired by simulated annealing and MoTIF poorly handles correlated atoms, simultaneously activated, or having varying amplitudes; some cases which often occur in practical applications.
% On the other hand,
% %
% these approaches are based on certain heuristics and often do not explicitly consider a mathematical objective, which limits their potential for possible extensions or improvements.
%  % therefore cannot be easily extended or improved due to this heuristic nature.
% \mjtodo{We should be careful with this statement. SWM does have a mathematical objective function that they minimize.}


 % it cannot be applied to signals where the atoms can be correlated, . 

% The MoTIF\cite{jost2006motif} algorithm, which has been successfully applied to estimate alpha waves from EEG, uses an iterative strategy based on generalized eigenvalue decompositions where the atoms are learned one at a time, under the constraint that successive atoms are orthogonal to each other. \mainak{This presents a limitation when recovering atoms that are correlated to each other.} More recently, the Sliding Window Matching (SWM) algorithms~\cite{gips2017discovering} were proposed to learn time-varying atoms from electrophysiological recordings. However, it is based on a quite ad-hoc inference technique which does not explicitly maximize a likelihood or minimize a cost function.
% As opposed to traditional dictionary learning in images, we are interested in learning time-invariant representations which are less redundant and more interpretable.

A natural way to cast the problem of learning a dictionary of shift-invariant atoms into an optimization problem is a \ac{CSC} approach~\citep{Grosse-etal:2007}. 
% The idea is to replace the matrix factorization of dictionary learning by a convolution operator~\cite{sejnowski1999coding} while keeping an $\ell_1$ or $\ell_0$ regularization to promote sparsity. 
This approach has gained popularity in computer vision~\citep{heide2015fast,wohlberg2016efficient,zeiler2010deconvolutional, vsorel2016fast,kavukcuoglu2010learning}%
, biomedical imaging~\citep{pachitariu2013extracting} and audio signal processing~\citep{Grosse-etal:2007,mailhe2008shift},
due to its ability to obtain compact representations of the signals and to incorporate the temporal structure of the signals via convolution.
%\mainak{Additionally, it comes with known theoretical guarantees~\cite{papyan2016working1,papyan2016working2}.
In the neuroscience context, \citet{barthelemy2013multivariate} used an extension of the K-SVD algorithm using convolutions on EEG data. 
% \utodo{Is this really CSC?}
In a similar spirit, \citet{brockmeier2016learning} used the matching pursuit algorithm combined with a rather heuristic  
% \mjtodo{(I feel a NIPS reviewer may not know MoTIF that well, so I wouldn't use "MoTIF-like")} 
dictionary update, which is similar to the MoTIF algorithm. 
%
In a very recent study, \citet{hitziger2017adaptive} proposed the AWL algorithm, which presents a mathematically more principled ac{CSC} approach for modeling neural signals. Yet, as opposed to classical ac{CSC} approaches, the AWL algorithm imposes additional combinatorial  constraints, which limit its scope to certain data that contain spike-like atoms. Also, since these constraints increase the complexity of the optimization problem, the authors had to resort to dataset-specific initializations and many heuristics in their inference procedure.
%The need for mathematically rigorous CSC tools for analyzing neural signals hence still prevails.  

%to decrease the objective function considered.


 % makes use of an $\ell_1$ regularization in a CSC model with some added combinatorial constraints. These make the optimization challenging and motivated a number of heuristics in the optimization schemes that \mainak{limit the scope of the model to data with spike-like atoms and} impact scalability and robustness of the approach. \ag{maybe too harsh...}. 
%Also since these approaches do not explicitly monitor a monotonically decreasing objective functions stopping criteria are unclear and solutions obtained with different initializations are not compared. Indeed, similar to the k-means algorithm, which too has a non-convex objective, CSC results can be significantly improved by using restart strategies.

% While this can be alleviated using a very good initialization~\cite{hitziger2017adaptive}, the shape of the atoms are not always known a priori.


While the current state-of-the-art \ac{CSC} methods have a strong potential for modeling neural signals, they might also be limited as they consider an $\ell_2$ reconstruction error, which corresponds to assuming an additive Gaussian noise distribution. While this assumption could be reasonable for several signal processing tasks, it turns out to be very restrictive for neural signals, which often contain heavy noise bursts and have low signal-to-noise ratio.

% While this is often good enough for computer vision applications where data are rarely corrupted by strong impulsive artifacts, it is not adapted to the demands of low signal-to-noise ratio in neural signals. 
%

In this study, we aim to address the aforementioned concerns and propose a novel probabilistic \ac{CSC} model called $\alpha$CSC, which is better-suited for neural signals. $\alpha$CSC is based on a family of \emph{heavy-tailed} distributions called $\alpha$-stable distributions \citep{samorodnitsky1994stable} whose rich structure covers a broad range of noise distributions. The heavy-tailed nature of the $\alpha$-stable distributions renders our model robust to impulsive observations. We develop a Monte Carlo expectation maximization (MCEM) algorithm for inference, with a weighted \ac{CSC} model for the maximization step. We propose efficient optimization strategies that are specifically designed for neural time series.
%
We illustrate the benefits of the proposed approach on both synthetic and real datasets.

% While such $\alpha$-stable distributions have been recently proposed for denoising EEG signals~\cite{wang2016delving} they have not been used for CSC in the presence of outliers and artifacts.



% To cope with the presence of transient and impulsive artifacts, heavy-tailed distribution such as $\alpha$-stable \cite{nolan:13} distributions are necessary. The $\alpha$-stable distributions generalize the Cauchy distribution and exhibit a slower tail decay in probability density function (PDF) than the Gaussian PDF.

% Standard dictionary learning in the presence of noise has been dealt with before using a robust loss~\cite{lu2013online}, but not in the convolutional setting.



% Besides these limitations of the current state-of-the-art, these model typically assume additive white noise with a Gaussian distribution. 
% % While this is often good enough for computer vision applications where data are rarely corrupted by strong impulsive artifacts, it is not adapted to the demands of low signal-to-noise ratio in neural signals. 
% %
% To cope with the presence of transient and impulsive artifacts, heavy-tailed distribution such as $\alpha$-stable \cite{nolan:13} distributions are necessary. The $\alpha$-stable distributions generalize the Cauchy distribution and exhibit a slower tail decay in probability density function (PDF) than the Gaussian PDF.
% While such $\alpha$-stable distributions have been recently proposed for denoising EEG signals~\cite{wang2016delving} they have not been used for CSC in the presence of outliers and artifacts. Standard dictionary learning in the presence of noise has been dealt with before using a robust loss~\cite{lu2013online}, but not in the convolutional setting.

% The sparsity constraint enables us to apply this method even on continuous recordings, not knowing a priori that the signal contains event-related potentials (ERPs) at specific time points. This is in contrast to convolutional models in fMRI event-related design~\cite{dale1999optimal} or the estimation of receptive fields~\cite{theunissen2001estimating} where the activations are already assumed to be known. If not for the sparsity constraint, one would obtain a convolutional Independent Component Analysis (ICA) or tensor factorization model\cite{huangconvolutional,truccolo2003estimation,morup2008shift}. Indeed, estimating single-trial ERPs containing onset jitters, has been solved many times before~\cite{woody1967characterization}, recently for example with the iterative least squares~\cite{barthelemy2013multivariate}. These methods learn not just the atoms but also the activations corresponding to the atoms and can even handle overlapping atoms, which is often necessary in neuroscience research~\cite{smith2015regression1,smith2015regression2}.

% The paper is organized as follows. First we present our probabilistic CSC model for which we derive an expectation-maximization (EM) inference algorithm. As the M-step leads to a standard CSC model using $\ell_2$ data fitting term, we then propose an alternated minimization scheme that we obtain by extension of state-of-the-art algorithmic solutions from computer vision literature. We then present results on simulations to demonstrate the better scalability of our implementation and the improved robustness to noise of our model. Practical impact of our contribution is then shown on 3 datasets.

% The optimization problem, as we frame it, is non-convex\cite{kavukcuoglu2010learning}, but it can be solved using an alternate minimization strategy. We enforce a non-negativity constraint on the activations to reflect the fact that atoms, due to their neuronal origin, cannot change their sign. The non-negativity, for us, also  allows a simpler interpretation, because it avoids cross-cancellation between atoms. Contrary to works in computer vision, the time series in our problem tend to be much longer which motivates us to solve the problem in the time domain. We compare our implementation to state-of-the-art implementations from computer vision in terms of performance. Finally, we demonstrate the power of our model on LFP data on rats which is known to have a cross-frequency coupling effect. Applying our method, the coupled waveform pops out almost effortlessly.