\chapter*{Abstract}

Electrophysiology experiments has for long relied upon small cohorts of subjects to uncover statistically significant effects of interest. However, the low sample size translates into a low power which leads to a high false discovery rate, and hence a low rate of reproducibility. To address this issue means solving two related problems: first, how do we  facilitate data sharing and reusability to build large datasets; and second, once big datasets are available, what tools can we build to analyze them?

In the first part of the thesis, we introduce a new data standard for sharing data known as the Brain Imaging Data Structure (BIDS), and its extension MEG-BIDS. Next, we introduce the reader to a typical electrophysiological pipeline analyzed with the MNE software package. This will orient them towards the analysis process and the challenges that are often faced when building reproducible pipelines. We consider the different choices that users have to deal with at each stage of the pipeline and provide standard recommendations.

Next, we focus on tools to automate analysis of large datasets such as those offered by the Human Connectome Project (HCP). We propose an automated tool to remove segments of data that are corrupted by artifacts. We develop an outlier detection algorithm based on tuning rejection thresholds with parameter search using Bayesian optimization. More importantly, we use the HCP data, which is manually annotated, to benchmark our algorithm against existing state-of-the-art methods. To our knowledge, this represents the first instance of reanalyzing the dataset using an independent stack of tools as used by the HCP consortium.

Finally, we use convolutional sparse coding to uncover structures in neural time series. The method we propose is inspired by similar algorithms in computer vision, where the goal is to learn the coefficients of a dictionary of atoms (traditionally sinusoidal or wavelets) but also the atoms themselves. We reformulate the existing approach as a \ac{MAP} inference to be able to deal with high amplitude artifacts and the heavy tailed noise distributions that is so common in neural time series. 

Taken together, this thesis represents an attempt to shift from slow and manual methods of analysis to automated, reproducible analysis.

\textbf{Keywords:} Automation, data sharing, reproducibility, sparse coding, outlier detection, representation learning, electroencephalography, magnetoencephalography