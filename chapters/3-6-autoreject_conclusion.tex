\section{Conclusion}

In summary, we have presented a novel algorithm for automatic data-driven detection and repair of bad segments in single trial M/EEG data. We therefore termed it \emph{autoreject}. We have compared our method to state-of-the-art methods on four different open datasets containing in total more than 200 subjects. Our validation suggests that \emph{autoreject} performs at least as good as diverse alternatives and commonly used procedures while often performing considerably better. This is the consequence of the combination of a data-driven outlier-detection step combined with physics-driven channel repair where all parameters are calibrated using a cross-validation strategy robust to outliers. The insight about the necessity to tune parameters at the level of single sensors and for individual subjects was further consolidated by our analyses of threshold distributions. The empirical variability of optimal thresholds across datasets emphasizes the importance of statistical learning approaches and automatic model selection strategies for preprocessing M/EEG signals. While \emph{autoreject} makes use of advanced statistical learning techniques such as Bayesian hyperparameter optimization, it is also grounded in the physics underlying the data generation. It is therefore not purely a black-box data-driven approach. It balances the trade-off between accuracy and interpretability. Indeed all \emph{autoreject} parameters have a meaning from a user standpoint and the algorithmic decisions can be explained. Supplemented by efficient diagnostic visualization routines, \emph{autoreject} can be easily integrated in MEG/EEG analysis pipelines,
including clinical ones where understanding algorithmic decisions is mandatory for tool adoption.
% accountability during scalable signal processing operations, something so urgently needed in the era of high-throughput neuroscience.
% \denis{another point got lost, in case of doubt a method that works in any context is better than one that is fast but does not generalize. Moreover we had some thoughts before machine learning VS classical evoked response estimation and I aergued that we still need classical methods in order to extract biomarkers or for some clinical assements, this would be valuable here}

By offering an automatic and data-driven algorithmic solution to a task mostly so far done manually, \emph{autoreject} reduces the cost of data inspection by experts. By allowing to repair data rather than removing it from the study, it allows saving data which are also costly to acquire. In addition, it removes the experts' bias which are due to specific training or prior experience, as well as some expectations about the data. It does so by defining a clear set of rules serving as inclusion criteria for M/EEG data, making results more easily reproducible and eventually limiting the risk of false discoveries. Furthermore, as data sharing across centers has become a common practice, \emph{autoreject}  addresses the issue of heterogeneous acquisition setups. Indeed, each acquisition set-up has its intrinsic signal qualities, which means that preprocessing parameters can vary significantly between datasets. As opposed to alternative methods, \emph{autoreject} automates the estimation of its parameters.
