\chapter{A reproducible M/EEG group study}

\section{Overview}

% General structure of each section (ideally)
% - Here is what we did
% - Why it makes sense? (figure)
% - This is how we checked that what we did makes sense

\Ac{MEG} and \ac{EEG} are neuroimaging technologies with a high temporal resolution, which provide non-invasive access to population-level neuronal dynamics on virtually any temporal scale currently considered relevant to cognition. % DE that is actually an important point to make. It's a distinct feature of the technology.
While MEG can recover spatial patterns at a higher \ac{SNR} and enjoys a more selective cortical resolution than \ac{EEG}~\citep{baillet17}, EEG is more portable and less expensive, and thus supports the study of cognition in a wider range of situations. Processing M/EEG recordings, however, is inherently challenging due to the multi-dimensional nature of the data, the low \ac{SNR} of brain-related M/EEG signals, and the differences in sensitivity of these measurement techniques. This can give rise to complex sequences of data processing steps which demand a high degree of organization from the investigator.

In an effort to address reproducibility issues recently shown to affect neuroimaging studies~\citep{ioannidis2005most, button2013power,Carp2012,Carp2012289}, a number of community-led efforts have begun developing data sharing~\citep{poldrack2017openfmri} and data organization~\citep{gorgolewski2016brain, galan2017meg} projects. These efforts are necessary first steps, but are not sufficient to solve the problem---they must be complemented by educational tools and guidelines that establish good practices for M/EEG analysis~\citep{gross-etal:13}. However, putting guidelines into practice is not always straightforward, as researchers in the M/EEG community rely on several software packages~\citep{tadel2011brainstorm,delorme2004eeglab,eeglab2,
oostenveld2010fieldtrip,nutmeg,litvak2011eeg}, each of which is different. Even though these packages provide tutorials for single subject data analysis, it is typically left up to the investigator to coordinate and implement multi-subject analyses. Here, we try to address this gap by demonstrating a principled approach to the assembly of group analysis pipelines with publicly available code\footnote{\url{https://github.com/mne-tools/mne-biomag-group-demo}} and extensive documentation. 

As members and maintainers within the MNE community, we will present analyses that make use of the MNE software suite~\citep{mne}. Historically, MNE was designed to calculate minimum-norm estimates from M/EEG data, and consisted in a collection of C-routines interfaced through bash shell scripts. Today, the MNE software has been reimplemented in~\citep{gramfort2013meg} and transformed into a general purpose toolbox for processing electrophysiology data. Built on top of a rich scientific ecosystem that is open source and free, MNE now offers state-of-the-art inverse solvers and tools for preprocessing, time-frequency analysis, machine learning (decoding and encoding), connectivity analysis, statistics, and advanced data visualization. MNE, moreover, has become a hub for researchers who use it as a platform to collaboratively develop novel methods or implement and disseminate the latest algorithms from the M/EEG community~\citep{engemann2015automated, smith2015regression1, smith2015regression2, haufe2014interpretation, king2014characterizing, gramfort-etal:2013, schurger2013reducing, khan2013note, larson_cortical_2012, hauk2011comparison, gramfort2010graph, rivet2009xdawn, kriegeskorte2008representational, maris_nonparametric_2007}. With this work, we not only  share best practices to facilitate reproducibility, but also present these latest advances in the MNE community which enable automation and quality assessment.

Here, we demonstrate how to use MNE to reanalyze the OpenfMRI dataset ds000117 by~\cite{wakeman2015multi}. This requires setting the objectives for the data analysis, breaking them down into separate steps and taking a series of decisions on how to handle the data at each of those steps. While there may be several interesting scientific questions that have not yet been addressed on this dataset, here we confine ourselves to the analysis of well-studied time-locked event-related M/EEG components, i.e, \acp{ERF} and \acp{ERP}. This is motivated by educational purposes to help facilitate comparisons between software packages and address reproducibility concerns. To this end, we will lay out all essential steps from single subject raw M/EEG recordings to group level statistics. Importantly, we will highlight the essential options, motivate our choices and point out important quality control objectives to evaluate the success of the analysis at every step.

We will first analyze the data in sensor space. We will discuss the best practices for selecting filter parameters, marking bad data segments, suppressing artifacts, epoching data into time windows of interest, averaging, and doing baseline correction. Next, we turn our attention to source localization: the various steps involved in the process starting from defining a head conductivity model, source space, coregistration of coordinate frames, data whitening, lead field computation, inverse solvers, and transformation of source-space data to a common space. Along the way, we will present various diagnostic visualization techniques that assist quality control at each processing step, such as channel-wise power spectral density (PSD), butterfly plots with spatial colors to facilitate readability, topographic maps, and whitening plots. Finally, we will attempt to distill from our analysis, guiding principles that should facilitate successfully designing \textit{other} reproducible analyses rather than blindly copying the recipes presented here. 
