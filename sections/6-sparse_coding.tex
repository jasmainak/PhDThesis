\chapter{Dictionary learning for M/EEG}

The next step in the thesis is to focus on unsupervised machine learning techniques for M/EEG using the large datasets already described in Chapter \ref{chapter:auto_reject}. The automated preprocessing pipeline we developed (Chapter \ref{chapter:auto_reject}) enables us to deploy such algorithms at scale. Inspired by efforts in the statistics community to automatically detect trends in data (see Automated Statistician~\citep{ghahramani2015probabilistic}), we hope to apply dictionary learning techniques to automatically detect primitives of the signal (known as atoms, basis vectors / functions, filters or kernels). Dictionary refers to the collection of atoms. In the context of M/EEG, this can be used to automatically extract neurologically plausible atoms which resemble evoked potentials and brain oscillations (alpha, beta waves). This will not only enable testing hypothesis such as the asymmetric nature of alpha waves~\citep{mazaheri2008asymmetric} but also allow automatically tagging the M/EEG  signal~\citep{bigdely2013hierarchical}. 

\section{Sparse Coding}
Historically, dictionary learning has been applied to natural images for extracting atoms resembling the receptive fields of V1 simple-cells~\citep{olshausen1997sparse}. In the most simple setting, we have $m$ (unlabeled) signals $x^{(i)} \in \real^{p}, i=1, ..., m$ each of which are decomposed as a linear combination of $n$ atoms $a^{(j)} \in \real^{p}, j=1, ..., n$ and some additive noise $\epsilon^{(i)}$.

\begin{equation}
   x^{(i)} = \sum_{j} s^{(i, j)} a^{(j)}  + \epsilon^{(i)}
\end{equation}

The objective of dictionary learning is to automatically learn the activations (also coefficients or code) $a^{(j)} \in \real^{p}, j=1, ..., n$ and optionally the atoms $s^{(i, j)}$. In the approach by \citet{olshausen1997sparse}, only the activations were learned from an overcomplete redundant dictionary. This ends up yielding an efficient representation of the data if a sparsity constraint is imposed on the activations. This means that a signal can be represented by only a few atoms in the dictionary. This approach to solving the dictionary learning problem is known as \textit{sparse coding}. It can be solved using a greedy algorithm such as matching pursuit which iteratively finds the atoms best matching the residual (signal not explained by the atoms so far). It can also be solved using the LASSO where the corresponding optimization problem is written down as:
%
\begin{eqnarray}
\min_{s, a} \sum_{i=1}^{m} \|x^{(i)} - \sum_{j} s^{(i, j)} a^{(j)} \|_{2}^{2} + \beta \sum_{i,j} \lvert s^{(i,j)} \rvert \\
s.t. \quad \| a^{(j)} \|^{2} \leq c, \quad 1 \leq j \leq n,
\end{eqnarray}
\label{eq:optim}
%
with the constraint being added to prevent trivial solutions.

\section{Convolutional sparse coding (CSC)}

In practice however, the entire image is divided into blocks (or patches for images) and each block is treated as a signal for the dictionary learning algorithm~\citep{mairal2009online}. A common criticism of this approach is that it learns redundant atoms. In the context of M/EEG, it  means that the same atom will be learned at different time delays. To fix this issue, time-invariant and convolutional sparse coding methods have been proposed. 

Whereas in block-based dictionary learning, the location is encoded in the block or patch, in convolutional sparse coding the location is encoded in the activations. One of the early attempts along this direction was by \citet{lewicki1999coding} applied to time-series data. In their approach, the atoms were replicated at each sample position such that $a^{(j)} \in \real^{p \times p}$ is a circulant matrix. This allowed them to account for time invariance. Their approach is based on maximizing the posterior distribution of the sparse code given the signal and atoms and assuming the noise to be Gaussian:

\begin{equation}
\hat{s} = \argmax_{s} P(s|x,a)
\end{equation}

The maximum is computed using conjugate gradient descent where the gradient is efficiently computed using convolutions. Sparsity is enforced heuristically by  pruning in each step of the optimization, coefficients which have a small contribution to the data fit. 

\subsection{CSC with dictionary updates}

Since then, more principled approaches for sparse coding problem have been proposed which solve the LASSO formulation in Equation~\ref{eq:optim} exactly. To enforce time-invariance, the optimization problem is recast as:

\begin{eqnarray}
\min_{s, a} \sum_{i=1}^{m} \|x^{(i)} - \sum_{j} s^{(i, j)} * a^{(j)} \|_{2}^{2} + \beta \sum_{i,j} \lvert s^{(i,j)} \rvert \\
s.t. \quad \| a^{(j)} \|^{2} \leq c, \quad 1 \leq j \leq n,
\end{eqnarray}
\label{eq:conv_optim}

\paragraph{Feature-sign trick:} Instead of expanding the convolutional operator and heuristically pruning the the coefficients, \citet{grosse2012shift} propose to use the ``feature-sign" trick~\citep{lee2006efficient}. In the ``feature-sign'' trick, the $L_{1}$-regularized least-squares problem reduces to an unconstrained quadratic optimization problem over just the non-zero coefficients if one can correctly guess the signs (positive, zero, or negative) of the optimal values of the coefficients. As the ``feature-sign" trick works efficiently only if the number of non-zero coefficients is less than 1000, they divide the signal into windows and refine the solution iteratively. In practice, only two iterations are sufficient for convergence making this algorithm efficient.~\citet{grosse2012shift} go one step further by \textit{learning the atoms} as well, instead of selecting atoms from an overcomplete dictionary\footnote{The dictionary learned using CSC is overcomplete too -- although much less than traditional sparse coding}. This is done by transforming the problem into the Fourier domain. The dual of the Fourier formulation can be decoupled into $p$ quadratic problems and thus solved efficiently. To wrap it up, the problem is solved by alternately optimizing for the basis and the activations. 

\paragraph{Predictive sparse coding:} Most attempts at solving the sparse coding problem now involved learning both the atoms and the activations. However, learning the activations efficiently was still challenging. \citet{kavukcuoglu2010learning} propose to learn the activations using predictive sparse coding~\citep{gregor2010learning} -- that is, train a parametrized non-linear ``encoder" function to predict optimal sparse codes. This turns out to be computationally 10 times faster compared to vanilla coordinate descent. The atoms are learned using stochastic gradient descent. An obvious shortcoming of this method is that it is supervised. Other solvers such as FISTA~\citep{beck2009fast} for the L1-constrained sparse coding problem have also been proposed~\citep{chalasani2013fast}.

\paragraph{Deconvolutional networks:} Another powerful approach is deconvolutional networks~\citep{zeiler2010deconvolutional}. These are similar to convolutional neural networks~\citep{lecun1989backpropagation} in terms of architecture, but the goal here is to learn high quality representations instead of predicting the outputs. At each layer, $m_{l-1}$ sparse codes are learned. Here the sparse code is learned layerwise by minimizing the cost function $C_l(x^{(i)})$ at layer $l$ starting from layer $l=1$ (inputs are the signal $x^{(i)}$). The cost function $C_l(x^{(i)})$ is given by:
%
\begin{equation}
C_l(x^{(i)}) = \sum_{k=1}^{m_{l-1}} \|s^{(i)}_{k, l-1} - \sum_{j=1}^{m_l} g_{l}^{(j,k)} (s^{(i)}_{j,l} * a^{(k)}_{j, l}) \|_{2}^{2} + \beta \sum_{j=1}^{m_l} \lvert s^{(i)}_{j,l} \rvert
\end{equation}
%
where $g_{l}^{(j,k)}$ determines the connectivity between sparse codes $s^{(i)}_{j,l}$ and $s^{(i)}_{k,l-1}$ in two consecutive layers. For $l=1$, we have $s^{(i)}_{k,0} = x^{(i)}$ and $g_{l}^{(j,k)}=1$.

The sparse coding problem is solved by introducing an auxiliary variable in the cost function and solving it by an augmented Lagrangian method. The atoms are learned using gradient descent. They show that the filters learned in the higher layers have higher encoding power with lower sparsity.

\paragraph{Handling boundary conditions:} Methods which transform the signal to Fourier domain (\textit{e.g.}, the one proposed by \citet{grosse2012shift}) to solve the CSC problem have the potential to introduce boundary artifacts. This is because the signal close to the boundary is covered by fewer atoms. \citet{heide2015fast} proposed a more general formulation of the CSC problem which allows them to handle boundary conditions properly:
%
\begin{eqnarray}
\min_{s, a} \sum_{i=1}^{m} \|x^{(i)} - M \sum_{j} s^{(i, j)} * a^{(j)} \|_{2}^{2} + \beta \sum_{i,j} \lvert s^{(i,j)} \rvert \\
s.t. \quad \| a^{(j)} \|^{2} \leq c, \quad 1 \leq j \leq n,
\end{eqnarray}
%
where $M$ masks out the boundaries of the estimation $\sum_{j} s^{(i,j)} * a^{(j)}$. The sparse approximation and dictionary update steps can both be split into two parts: one involving the sparse approximation or dictionary update, and the other involving the mask operator $M$. This can be solved using the alternating direction method of multipliers (ADMM). The final algorithm involves an outer loop of coordinate descent alternating between the dictionary update step and the sparse approximation. 

In the formulation of \citet{heide2015fast}, each step of the ADMM algorithm requires solving a linear system of equations in the Fourier domain, which is $O(Z^3)$. \citet{wohlberg2016efficient} proposed a more efficient implementation by exploiting the unique structure of the problem: the left hand side of the equations is the sum of a diagonal and a rank-one matrix. Instead of solving the linear system of equations (in the Fourier domain) by Gaussian elimination, \cite{wohlberg2016efficient} uses the Sherman-Morrison formula. Although it is slightly less precise compared to Gaussian elimination, in practice it is a reasonable trade off as the ADMM algorithm does not require a very accurate solution in each step. In another study,~\citet{vsorel2016fast} use the same trick to propose a faster algorithm for CSC.

\section{Dictionary learning in M/EEG}

In the case of M/EEG, the models described so far are reasonable only for a single channel $c$, that is $x^{(i)}$ in Equation~\ref{eq:conv_optim} should in fact be $x^{(i)}_{c}$ with $c=1,..,C$ for the M/EEG case. Reasonable efforts along this direction have already been made. 

\subsection{Single channel model}

\citet{jost2006motif} were able to apply a time-invariant dictionary learning technique to learn atoms resembling alpha and beta waves in EEG signal from a single channel. Consensus matching pursuit \citep{benar2009consensus} involves learning atoms for each trial and retaining those which are similar in the time-frequency domain.

\subsection{Multichannel model}
\label{sec:multichannel}
\citet{durka2005multichannel} used multichannel matching pursuit (MMP) for automatic detection and parametrization of sleep spindles. In their approach, they fixed the phases of the Gabor atoms across channels as the method proposed was intended as a preprocessing step for EEG inverse solutions. They hint that allowing for phase jitter between channels might be desirable as they might reveal the direction of information flow between channels. In fact, the phase relaxation for MMP was implemented by \citet{sieluzyckiamultivariate}. They modeled the M100 response in MEG using this approach obtaining a lower reconstruction error. However, the authors are unable to conclude whether it reflects the real trial-to-trial variation in phase or particular noise patterns. In another study, the MMP method (fixed phase) was applied to multiple trials instead of multiple channels \citep{sieluzycki2009single}. This method was able to extract trial-to-trial to variations in the M100 response and the authors were able to show habituation effects at the level of single trials.

\subsection{Multivariate model}
Contrary to multichannel methods, here the assumption of the atoms being spread across channels is relaxed. \citet{barthelemy2013multivariate} proposed using a multivariate extension of the method proposed by~\cite{durka2005multichannel}, which does not force the same atoms to have the same polarities across channels. Instead, a spatiotemporal atom is learnt which is much more richer as compared to the multichannel model. The method works by alternating between a sparse approximation step and a multivariate dictionary update. The sparse approximation is carried out as in Equation~\ref{eq:conv_optim}. The authors report higher performance as compared to the multichannel model (Section~\ref{sec:multichannel}). However, it is not clear if the gains are due to the dictionary update step (which the multichannel models proposed so far lack) or due to the multivariate model. Intuitively, the multichannel models should perform better than the multivariate model.

\section{Challenges and future directions}

To summarize, the key ingredients for dictionary learning in M/EEG are --

\begin{itemize}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\item Atoms should be time-invariant or convolutional
\item The atoms must linearly mix in all the channels
\item Imposes a sparsity penalty to reduce redundancy
\item The atoms are learned rather than selected from an overcomplete dictionary
\end{itemize}

The multichannel model with rank-1 spatiotemporal dictionaries (i.e., the atoms linearly mix in all channels) appears to be suitable for modeling the physical aspect of the problem. The algorithms by ~\citet{wohlberg2016efficient} appears to be the most promising in terms of learning the atoms as it can handle boundary conditions while also being efficient and convolutional. The challenge will be to deploy such algorithms across many subjects and at a large scale, for example in the case of Human Connectome Project~\citep{van2012human}. Learning them efficiently will involve out-of-core algorithms. This is part of future work.
